{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"To36tSvvu1Rk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732109027051,"user_tz":-300,"elapsed":6930,"user":{"displayName":"usman","userId":"04999818267277661519"}},"outputId":"8491741e-fff5-4ca6-813f-fbaee617253e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Collecting datasets\n","  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec (from torch)\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"]}],"source":["!pip install transformers torch datasets pandas numpy"]},{"cell_type":"markdown","source":["## Feature Extraction"],"metadata":{"id":"mVe0OgqVzf5c"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader\n","from transformers import BertTokenizer, BertModel\n","from datasets import load_dataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Initialize the tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Add a padding token if not present\n","if tokenizer.pad_token is None:\n","    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","    model = BertModel.from_pretrained('bert-base-uncased')\n","    model.resize_token_embeddings(len(tokenizer))\n","else:\n","    model = BertModel.from_pretrained('bert-base-uncased')\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Move the model to GPU if available\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","model.to(device)\n","\n","print(f'Using device: {device}')\n","\n","# Load the IMDb dataset\n","dataset = load_dataset('imdb')\n","\n","# Select the training set\n","train_dataset = dataset['train']\n","\n","# Display the first example\n","print(train_dataset[0])\n","\n","# Define the maximum sequence length\n","max_length = 128\n","\n","# Tokenization function\n","def tokenize(batch):\n","    return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=max_length)\n","\n","# Apply the tokenization to the dataset\n","tokenized_train = train_dataset.map(tokenize, batched=True, batch_size=32)\n","\n","# Inspect the tokenized data\n","print(tokenized_train.features)\n","\n","# Set format for PyTorch\n","tokenized_train.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n","\n","# Define batch size\n","batch_size = 32\n","\n","# Create DataLoader\n","train_loader = DataLoader(tokenized_train, batch_size=batch_size, shuffle=False)\n","\n","# Function to extract features\n","def extract_features(loader, model, device):\n","    all_features = []\n","    all_labels = []\n","\n","    # Disable gradient calculations for efficiency\n","    with torch.no_grad():\n","        for batch in tqdm(loader, desc=\"Extracting Features\"):\n","            # Move inputs to the device\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","\n","            # Get the model outputs\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","\n","            # Extract the [CLS] token's embedding\n","            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n","            all_features.append(cls_embeddings)\n","\n","            # Collect labels\n","            all_labels.extend(batch['label'].cpu().numpy())\n","\n","    # Concatenate all features\n","    all_features = np.concatenate(all_features, axis=0)\n","    all_labels = np.array(all_labels)\n","\n","    return all_features, all_labels\n","\n","# Extract features from the tokenized training set\n","features, labels = extract_features(train_loader, model, device)\n","\n","print(f'Feature shape: {features.shape}')  # (25000, 768)\n","print(f'Labels shape: {labels.shape}')      # (25000,)\n","\n","# Split into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, random_state=42)\n","\n","print(f'Training set size: {X_train.shape}')\n","print(f'Validation set size: {X_val.shape}')\n","\n","# Initialize the classifier\n","classifier = LogisticRegression(max_iter=1000)\n","\n","# Train the classifier\n","print(\"Training the classifier...\")\n","classifier.fit(X_train, y_train)\n","\n","# Predict on the validation set\n","y_pred = classifier.predict(X_val)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_val, y_pred)\n","print(f'Validation Accuracy: {accuracy * 100:.2f}%')\n","\n","# Detailed classification report\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_val, y_pred, target_names=['Negative', 'Positive']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eqsjc5Enxkq2","executionInfo":{"status":"ok","timestamp":1732109746136,"user_tz":-300,"elapsed":181345,"user":{"displayName":"usman","userId":"04999818267277661519"}},"outputId":"ca8bb08f-7f1c-4370-93da-b19c4355db23"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n","{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"]},{"output_type":"stream","name":"stderr","text":["Extracting Features: 100%|██████████| 782/782 [02:48<00:00,  4.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Feature shape: (25000, 768)\n","Labels shape: (25000,)\n","Training set size: (20000, 768)\n","Validation set size: (5000, 768)\n","Training the classifier...\n","Validation Accuracy: 81.08%\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.81      0.82      0.81      2515\n","    Positive       0.81      0.81      0.81      2485\n","\n","    accuracy                           0.81      5000\n","   macro avg       0.81      0.81      0.81      5000\n","weighted avg       0.81      0.81      0.81      5000\n","\n"]}]},{"cell_type":"markdown","source":["## Testing"],"metadata":{"id":"lW9as0hCzbjj"}},{"cell_type":"code","source":["import torch\n","from transformers import BertTokenizer, BertModel\n","import pickle\n","import numpy as np\n","\n","\n","def predict_label(text, tokenizer, model, classifier, device, max_length=128):\n","    \"\"\"\n","    Predict the label of a given text using the trained classifier and BERT embeddings.\n","\n","    Args:\n","        text (str): Input text for which to predict the label.\n","        tokenizer: Loaded tokenizer.\n","        model: Loaded BERT model.\n","        classifier: Loaded classifier.\n","        device: Device where the model is loaded (CPU or GPU).\n","        max_length (int): Maximum sequence length for BERT.\n","\n","    Returns:\n","        label (str): Predicted label ('Negative' or 'Positive').\n","    \"\"\"\n","    # Tokenize the input text\n","    inputs = tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n","\n","    # Move inputs to the device\n","    input_ids = inputs['input_ids'].to(device)\n","    attention_mask = inputs['attention_mask'].to(device)\n","\n","    # Extract BERT embeddings\n","    with torch.no_grad():\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # Shape: (1, 768)\n","\n","    # Predict using the classifier\n","    predicted_label = classifier.predict(cls_embedding)[0]\n","\n","    # Map numerical label to string\n","    label_mapping = {0: 'Negative', 1: 'Positive'}\n","    return label_mapping.get(predicted_label, 'Unknown')\n","\n","# Example Usage:\n","if __name__ == \"__main__\":\n","    # Load components\n","    # tokenizer, model, classifier, device = load_components()\n","\n","    # Input texts\n","    texts = [\n","        \"I absolutely loved this movie! The performances were outstanding.\",\n","        \"This was the worst film I have ever seen. It was a complete waste of time.\",\n","        \"An average movie with some good moments but overall not impressive.\",\n","    ]\n","\n","    # Predict labels\n","    for text in texts:\n","        label = predict_label(text, tokenizer, model, classifier, device)\n","        print(f\"Text: {text}\\nPredicted Label: {label}\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yfsx72zayYN9","executionInfo":{"status":"ok","timestamp":1732110017219,"user_tz":-300,"elapsed":392,"user":{"displayName":"usman","userId":"04999818267277661519"}},"outputId":"650e9e50-d1f5-446a-e01d-ee3beb20adc4"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Text: I absolutely loved this movie! The performances were outstanding.\n","Predicted Label: Positive\n","\n","Text: This was the worst film I have ever seen. It was a complete waste of time.\n","Predicted Label: Negative\n","\n","Text: An average movie with some good moments but overall not impressive.\n","Predicted Label: Positive\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"kwXhQZonzV7o"},"execution_count":null,"outputs":[]}]}